"""
Lumina å¿ƒèª - ç™‚è‚²éŸ³é »ç”Ÿæˆæ¨¡çµ„ v2.0
å¯¦ä½œã€Œåˆ†æ®µç”Ÿæˆèˆ‡è‡ªå‹•ä¸²æ¥ã€é‚è¼¯ï¼Œè§£æ±º TTS API è¼¸å‡ºé•·åº¦é™åˆ¶

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. script_splitter - æ ¹æ“š [PART_X] æ¨™ç±¤æ‹†åˆ†æ–‡ç¨¿
2. é †åºç”Ÿæˆæ¯å€‹ç‰‡æ®µçš„éŸ³é »
3. ä½¿ç”¨ pydub ç„¡ç¸«ç¸«åˆæ‰€æœ‰ç‰‡æ®µ
"""

import os
import re
import wave
import base64
from io import BytesIO
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple
from google import genai
from google.genai import types

# å˜—è©¦å°å…¥ pydubï¼Œå¦‚æœå¤±æ•—å‰‡ä½¿ç”¨ç´” WAV æ‹¼æ¥
try:
    from pydub import AudioSegment
    PYDUB_AVAILABLE = True
except ImportError:
    PYDUB_AVAILABLE = False
    print("âš ï¸ pydub æœªå®‰è£ï¼Œå°‡ä½¿ç”¨åŸºç¤ WAV æ‹¼æ¥")

# æ¨¡å‹å¸¸é‡
TTS_MODEL = "gemini-2.5-flash-preview-tts"  # TTS å°ˆç”¨æ¨¡å‹
TEXT_MODEL = "gemini-2.5-flash"  # ç”¨æ–¼ç”Ÿæˆæ–‡ç¨¿

# å¯ç”¨çš„è²éŸ³é¸é … (ä¸­æ–‡æ¨è–¦ä½¿ç”¨ Kore æˆ– Aoede)
VOICE_OPTIONS = {
    "warm_female": "Kore",      # æº«æš–å¥³è²
    "calm_female": "Aoede",     # å¹³éœå¥³è²
    "gentle_male": "Charon",    # æº«å’Œç”·è²
    "soothing_male": "Fenrir",  # èˆ’ç·©ç”·è²
}


def split_script_by_parts(script: str) -> List[Tuple[str, str]]:
    """
    æ ¹æ“š [PART_X] æ¨™ç±¤å°‡è…³æœ¬æ‹†åˆ†ç‚ºå¤šå€‹ç‰‡æ®µ
    
    Args:
        script: åŒ…å« [PART_1], [PART_2] ç­‰æ¨™ç±¤çš„å®Œæ•´è…³æœ¬
        
    Returns:
        List of tuples: [(part_name, content), ...]
    """
    # åŒ¹é… [PART_X] æ ¼å¼çš„æ¨™ç±¤
    pattern = r'\[PART_(\d+)\]'
    
    # æ‰¾åˆ°æ‰€æœ‰æ¨™ç±¤çš„ä½ç½®
    matches = list(re.finditer(pattern, script))
    
    if not matches:
        # å¦‚æœæ²’æœ‰æ¨™ç±¤ï¼Œè¿”å›æ•´å€‹è…³æœ¬ä½œç‚ºå–®ä¸€ç‰‡æ®µ
        print("âš ï¸ æœªæ‰¾åˆ° [PART_X] æ¨™ç±¤ï¼Œå°‡æ•´é«”ä½œç‚ºå–®ä¸€ç‰‡æ®µè™•ç†")
        return [("PART_1", script.strip())]
    
    parts = []
    for i, match in enumerate(matches):
        part_name = f"PART_{match.group(1)}"
        start = match.end()
        
        # çµæŸä½ç½®æ˜¯ä¸‹ä¸€å€‹æ¨™ç±¤çš„é–‹å§‹ï¼Œæˆ–è€…æ˜¯æ–‡æœ¬æœ«å°¾
        if i + 1 < len(matches):
            end = matches[i + 1].start()
        else:
            end = len(script)
        
        content = script[start:end].strip()
        if content:  # åªæ·»åŠ éç©ºå…§å®¹
            parts.append((part_name, content))
    
    print(f"ğŸ“[Script Splitter] æˆåŠŸæ‹†åˆ†ç‚º {len(parts)} å€‹ç‰‡æ®µ")
    for name, content in parts:
        print(f"   - {name}: {len(content)} å­—")
    
    return parts


class StreamingBGMMixer:
    """
    ä¸²æµ BGM æ··åˆå™¨ï¼šé‚Šç”Ÿæˆé‚Šæ··åˆèƒŒæ™¯éŸ³æ¨‚
    
    è¨­è¨ˆï¼š
    1. åˆå§‹åŒ–æ™‚è¼‰å…¥ä¸¦æº–å‚™ä¸€å€‹è¶³å¤ é•·çš„ BGM loop
    2. è¿½è¹¤ç•¶å‰ BGM æ’­æ”¾ä½ç½®ï¼ˆæ¯«ç§’ï¼‰
    3. æ¯å€‹ TTS ç‰‡æ®µç”Ÿæˆå¾Œï¼Œå¾ BGM ä¸­è£å‰ªå°æ‡‰é•·åº¦çš„ç‰‡æ®µ
    4. æ··åˆ TTS ç‰‡æ®µå’Œ BGM ç‰‡æ®µ
    5. æ›´æ–° BGM ä½ç½®æŒ‡é‡
    
    ç”¨æ³•ï¼š
        mixer = StreamingBGMMixer(stage2_result)
        if mixer.is_ready:
            for tts_audio in tts_parts:
                mixed_audio = mixer.mix_segment(tts_audio)
                yield mixed_audio
    """
    
    def __init__(self, stage2_result: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–ä¸²æµæ··åˆå™¨
        
        Args:
            stage2_result: ç”¨æ–¼æå–æƒ…ç·’æ¨™ç±¤ä»¥é¸æ“‡åˆé©çš„ BGM
        """
        self.is_ready = False
        self.bgm_audio = None
        self.bgm_position_ms = 0  # ç•¶å‰ BGM ä½ç½®ï¼ˆæ¯«ç§’ï¼‰
        self.bgm_volume_reduction = -20  # BGM é™ä½ 20dB
        self.fade_duration_ms = 500  # ç‰‡æ®µé–“æ·¡å…¥æ·¡å‡º
        self.bgm_path = None
        
        try:
            from pydub import AudioSegment
            from conflict_analyzer.audio_mixer import AudioMixer
            
            self.AudioSegment = AudioSegment
            
            # åˆå§‹åŒ– AudioMixer ä»¥é¸æ“‡ BGM
            mixer = AudioMixer()
            
            # å¾ stage2 æå–æƒ…ç·’
            emotion = "healing"
            if isinstance(stage2_result, dict):
                if stage2_result.get("sentiment_vibe"):
                    emotion = stage2_result["sentiment_vibe"]
            
            # é¸æ“‡ BGM
            self.bgm_path = mixer.select_bgm(emotion)
            
            if self.bgm_path:
                # è¼‰å…¥ BGM
                self.bgm_audio = AudioSegment.from_file(str(self.bgm_path))
                
                # é™ä½ BGM éŸ³é‡
                self.bgm_audio = self.bgm_audio + self.bgm_volume_reduction
                
                # ç¢ºä¿ BGM è¶³å¤ é•·ï¼ˆè‡³å°‘ 10 åˆ†é˜ï¼‰
                target_duration_ms = 10 * 60 * 1000  # 10 åˆ†é˜
                if len(self.bgm_audio) < target_duration_ms:
                    # å¾ªç’°æ‹¼æ¥
                    loops_needed = (target_duration_ms // len(self.bgm_audio)) + 1
                    self.bgm_audio = self.bgm_audio * loops_needed
                
                self.is_ready = True
                print(f"   ğŸµ [StreamingBGMMixer] BGM å·²è¼‰å…¥: {self.bgm_path.name}")
                print(f"   ğŸµ [StreamingBGMMixer] BGM ç¸½æ™‚é•·: {len(self.bgm_audio) / 1000:.1f} ç§’")
            else:
                print("   âš ï¸ [StreamingBGMMixer] æ²’æœ‰å¯ç”¨çš„ BGM æ–‡ä»¶")
                
        except ImportError as e:
            print(f"   âš ï¸ [StreamingBGMMixer] åˆå§‹åŒ–å¤±æ•—: {e}")
        except Exception as e:
            print(f"   âš ï¸ [StreamingBGMMixer] è¼‰å…¥ BGM å¤±æ•—: {e}")
    
    def mix_segment(self, voice_audio: bytes, voice_format: str = "wav") -> bytes:
        """
        å°‡å–®å€‹ TTS ç‰‡æ®µèˆ‡ BGM æ··åˆ
        
        Args:
            voice_audio: TTS ç”Ÿæˆçš„èªéŸ³ bytes
            voice_format: èªéŸ³æ ¼å¼
            
        Returns:
            æ··åˆå¾Œçš„éŸ³é » bytes
        """
        if not self.is_ready or not self.bgm_audio:
            return voice_audio  # ç„¡ BGMï¼Œè¿”å›åŸéŸ³é »
        
        try:
            from io import BytesIO
            
            # è¼‰å…¥èªéŸ³ç‰‡æ®µ
            voice_buffer = BytesIO(voice_audio)
            voice_segment = self.AudioSegment.from_file(voice_buffer, format=voice_format)
            voice_duration_ms = len(voice_segment)
            
            # å¾ BGM ä¸­è£å‰ªå°æ‡‰ä½ç½®çš„ç‰‡æ®µ
            bgm_start = self.bgm_position_ms
            bgm_end = bgm_start + voice_duration_ms
            
            # ç¢ºä¿ä¸è¶…å‡º BGM é•·åº¦ï¼ˆå¾ªç’°ï¼‰
            if bgm_end > len(self.bgm_audio):
                # BGM å·²æ’­æ”¾å®Œï¼Œå¾é ­å¾ªç’°
                self.bgm_position_ms = 0
                bgm_start = 0
                bgm_end = voice_duration_ms
            
            bgm_segment = self.bgm_audio[bgm_start:bgm_end]
            
            # æ›´æ–° BGM ä½ç½®
            self.bgm_position_ms = bgm_end
            
            # æ··åˆ
            mixed = bgm_segment.overlay(voice_segment, position=0)
            
            # è¼¸å‡º
            output_buffer = BytesIO()
            mixed.export(output_buffer, format="wav")
            output_buffer.seek(0)
            
            return output_buffer.read()
            
        except Exception as e:
            print(f"   âš ï¸ [StreamingBGMMixer] æ··åˆç‰‡æ®µå¤±æ•—: {e}")
            return voice_audio  # å¤±æ•—æ™‚è¿”å›åŸéŸ³é »
    
    def get_status(self) -> Dict[str, Any]:
        """ç²å–æ··åˆå™¨ç‹€æ…‹"""
        return {
            "is_ready": self.is_ready,
            "bgm_file": self.bgm_path.name if self.bgm_path else None,
            "current_position_ms": self.bgm_position_ms,
            "method": "streaming_local" if self.is_ready else "none"
        }


class HealingAudioGenerator:
    """ç”Ÿæˆç™‚è‚²éŸ³é »çš„æ ¸å¿ƒé¡ï¼ˆæ”¯æ´åˆ†æ®µç”Ÿæˆèˆ‡ä¸²æ¥ï¼‰"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("éœ€è¦ GEMINI_API_KEY ç’°å¢ƒè®Šæ•¸")
        self.client = genai.Client(api_key=self.api_key)
    
    def generate_healing_script(
        self,
        stage1_result: Dict[str, Any],
        stage2_result: Dict[str, Any],
        stage3_result: Dict[str, Any],
        system_prompt: str,
        additional_context: str = ""
    ) -> str:
        """
        ç”Ÿæˆç™‚è‚²éŸ³é »æ–‡ç¨¿ï¼ˆå¸¶æœ‰ [PART_X] æ¨™ç±¤ï¼‰
        
        Returns:
            åŒ…å« [PART_1], [PART_2], ... æ¨™ç±¤çš„çµæ§‹åŒ–ç™‚è‚²è…³æœ¬
        """
        from conflict_analyzer.prompts import get_stage4_prompt
        
        print("ğŸ“[HealingAudioGenerator] æ­£åœ¨ç”Ÿæˆåˆ†æ®µç™‚è‚²æ–‡ç¨¿...")
        
        user_prompt = get_stage4_prompt(
            stage1_result, 
            stage2_result, 
            stage3_result,
            additional_context
        )
        
        try:
            response = self.client.models.generate_content(
                model=TEXT_MODEL,
                contents=user_prompt,
                config=types.GenerateContentConfig(
                    system_instruction=system_prompt,
                    temperature=0.8,
                )
            )
            
            script = response.text.strip()
            print(f"ğŸ“[HealingAudioGenerator] æ–‡ç¨¿ç”ŸæˆæˆåŠŸï¼ç¸½é•·åº¦: {len(script)} å­—")
            return script
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆç™‚è‚²æ–‡ç¨¿éŒ¯èª¤: {e}")
            raise
    
    def _build_healing_tts_prompt(self, text: str, part_name: str = "") -> str:
        """
        æ§‹å»ºå¸¶æƒ…ç·’æ§åˆ¶çš„ TTS Promptï¼ˆä½¿ç”¨ Google å®˜æ–¹æ¨è–¦çš„ Audio Profile æ ¼å¼ï¼‰
        
        ç™‚è‚²éŸ³é »å°ˆç”¨æŒ‡ä»¤ï¼šæº«æš–ã€åŒç†å¿ƒã€ç·©æ…¢å‘¼å¸å¼åœé “
        
        Args:
            text: è¦æœ—è®€çš„æ–‡å­—
            part_name: ç‰‡æ®µåç¨±ï¼ˆå¦‚ PART_1, PART_2ï¼‰
            
        Returns:
            å¸¶é¢¨æ ¼æŒ‡ä»¤çš„å®Œæ•´ prompt
        """
        # æ ¹æ“šç‰‡æ®µåç¨±èª¿æ•´æƒ…ç·’
        emotion_guide = self._get_emotion_for_part(part_name)
        
        # æ§‹å»ºå°ˆæ¥­çš„ Audio Profile
        prompt = f"""# AUDIO PROFILE: ç™‚è‚²å¼•å°å¸«
## "Healing Voice Guide"

## THE SCENE:
ä¸€å€‹å¯§éœçš„ç™‚ç™’ç©ºé–“ï¼ŒæŸ”å’Œçš„ç‡ˆå…‰ç‘è½ã€‚
è†è½è€…æ­£è™•æ–¼ä¸€å€‹å®‰å…¨ã€è¢«æ¥ç´çš„ç’°å¢ƒä¸­ã€‚
é€™æ˜¯ä¸€æ®µç§å¯†çš„è‡ªæˆ‘ç™‚ç™’æ™‚åˆ»ã€‚

### DIRECTOR'S NOTES

**Style:** {emotion_guide['style']}

**Pacing:** {emotion_guide['pacing']}

**Breathing:** æ¯å€‹å¥å­çµæŸå¾Œç•™ä¸‹è‡ªç„¶çš„å‘¼å¸ç©ºé–“ã€‚
åœ¨é€—è™Ÿå’Œå¥è™Ÿè™•é©ç•¶åœé “ï¼Œè®“è†è½è€…æœ‰æ™‚é–“å¸æ”¶å’Œæ„Ÿå—ã€‚
ä¸è¦æ€¥èºï¼Œè®“æ¯å€‹å­—éƒ½å¸¶è‘—æº«åº¦ç·©ç·©æµå‡ºã€‚

**Emotional Arc:** {emotion_guide['emotional_arc']}

**Voice Quality:** 
- ä½¿ç”¨ã€Œè²éŸ³å¾®ç¬‘ã€æŠ€å·§ï¼Œè®“èªèª¿å¸¶è‘—æº«æš–
- ä¿æŒä½æ²‰ä½†æ¸…æ™°çš„éŸ³èª¿
- é¿å…éåº¦æˆ²åŠ‡åŒ–ï¼Œä¿æŒçœŸèª è‡ªç„¶

## TRANSCRIPT:
{text}
"""
        return prompt
    
    def _get_emotion_for_part(self, part_name: str) -> dict:
        """
        æ ¹æ“šç‰‡æ®µåç¨±è¿”å›å°æ‡‰çš„æƒ…ç·’æŒ‡å°
        
        ç™‚è‚²éŸ³é »çš„æƒ…ç·’æ›²ç·šï¼šé–‹å ´ â†’ å…±æƒ… â†’ æ·±å…¥ â†’ è½‰åŒ– â†’ å¸Œæœ›
        """
        part_emotions = {
            "PART_1": {
                "style": "æº«æš–è€Œå¯Œæœ‰åŒç†å¿ƒçš„é–‹å ´ç™½ã€‚åƒä¸€ä½å€¼å¾—ä¿¡è³´çš„æœ‹å‹ï¼Œè¼•æŸ”åœ°å•å€™ã€‚èªèª¿è¦è®“äººæ„Ÿåˆ°è¢«ç†è§£ã€è¢«æ¥ç´ã€‚",
                "pacing": "ç·©æ…¢è€Œç©©å®šï¼Œçµ¦äºˆå……è¶³çš„ç©ºé–“ã€‚æ¯åˆ†é˜ç´„ 100-120 å­—ã€‚",
                "emotional_arc": "å¾å¹³éœé–‹å§‹ï¼Œé€æ¼¸å»ºç«‹ä¿¡ä»»æ„Ÿã€‚"
            },
            "PART_2": {
                "style": "æ·±åº¦å…±æƒ…å’Œç†è§£ã€‚æ‰¿èªç—›è‹¦çš„å­˜åœ¨ï¼Œä¸è©¦åœ–ç«‹å³ä¿®å¾©ã€‚èªèª¿è¦å‚³é”ã€Œæˆ‘ç†è§£ä½ ã€çš„è¨Šæ¯ã€‚",
                "pacing": "ç¨æ…¢ï¼Œåœ¨é‡è¦çš„æƒ…æ„Ÿè©å½™å‰å¾Œç•™ä¸‹åœé “ã€‚",
                "emotional_arc": "æ·±å…¥é€£çµï¼Œè®“è†è½è€…æ„Ÿåˆ°è¢«çœ‹è¦‹ã€‚"
            },
            "PART_3": {
                "style": "æ´å¯Ÿèˆ‡å•Ÿç™¼ã€‚å¸¶æœ‰ä¸€çµ²å¥½å¥‡å’Œæ¢ç´¢çš„èªèª¿ã€‚è¼•è¼•å¼•å°è†è½è€…çœ‹è¦‹æ–°çš„è§’åº¦ã€‚",
                "pacing": "ä¸­ç­‰é€Ÿåº¦ï¼Œåœ¨é—œéµæ´å¯Ÿè™•é©ç•¶åŠ é‡èªæ°£ã€‚",
                "emotional_arc": "å¾ç†è§£åˆ°é ˜æ‚Ÿçš„è½‰æŠ˜é»ã€‚"
            },
            "PART_4": {
                "style": "å¸Œæœ›èˆ‡åŠ›é‡ã€‚èªèª¿é€æ¼¸è®Šå¾—æ˜äº®ã€å …å®šä½†ä¸å¤±æº«æŸ”ã€‚å‚³é”ã€Œä½ å¯ä»¥çš„ã€çš„ä¿¡å¿µã€‚",
                "pacing": "ç•¥å¾®åŠ å¿«ï¼Œä½†ä¿æŒç©©å®šå’Œè‡ªä¿¡ã€‚",
                "emotional_arc": "å‘ä¸Šæšèµ·ï¼Œæ³¨å…¥å¸Œæœ›å’Œèƒ½é‡ã€‚"
            },
            "PART_5": {
                "style": "æº«æš–çš„ç¥ç¦å’Œæ”¶å°¾ã€‚åƒä¸€å€‹æº«æŸ”çš„æ“æŠ±ï¼Œå¸¶è‘—ç¥ç¦é€åˆ¥ã€‚",
                "pacing": "å›æ­¸ç·©æ…¢ï¼Œè®“æœ€å¾Œçš„è©±èªæ²‰æ¾±åœ¨å¿ƒä¸­ã€‚",
                "emotional_arc": "å¹³éœæ”¶å°¾ï¼Œç•™ä¸‹æŒä¹…çš„æº«æš–ã€‚"
            }
        }
        
        # å˜—è©¦åŒ¹é…ç‰‡æ®µåç¨±
        for key in part_emotions:
            if key in part_name.upper():
                return part_emotions[key]
        
        # é è¨­æƒ…ç·’ï¼ˆé©ç”¨æ–¼æœªçŸ¥ç‰‡æ®µï¼‰
        return {
            "style": "æº«æš–ã€å¯Œæœ‰åŒç†å¿ƒã€çœŸèª è‡ªç„¶ã€‚åƒä¸€ä½æ™ºæ…§çš„ç™‚ç™’å¸«ï¼Œç”¨å¿ƒå‚¾è¯ä¸¦æº«æŸ”å›æ‡‰ã€‚",
            "pacing": "ç·©æ…¢è€Œç©©å®šï¼Œæ¯åˆ†é˜ç´„ 100-120 å­—ã€‚è‡ªç„¶çš„å‘¼å¸å¼åœé “ã€‚",
            "emotional_arc": "ä¿æŒå¹³ç©©æº«æš–ï¼Œå‚³é”æ”¯æŒèˆ‡ç†è§£ã€‚"
        }

    def text_to_speech_single(
        self,
        text: str,
        voice: str = "warm_female",
        part_name: str = "",
        max_retries: int = 3  # æ–°å¢ï¼šæœ€å¤§é‡è©¦æ¬¡æ•¸
    ) -> bytes:
        """
        å°‡å–®ä¸€ç‰‡æ®µæ–‡å­—è½‰æ›ç‚ºèªéŸ³ï¼ˆå¸¶æŒ‡æ•¸é€€é¿é‡è©¦æ©Ÿåˆ¶ï¼‰
        
        Args:
            text: è¦è½‰æ›çš„æ–‡å­—ï¼ˆæ‡‰æ§åˆ¶åœ¨ 200 å­—ä»¥å…§ï¼‰
            voice: è²éŸ³é¸é …
            part_name: ç‰‡æ®µåç¨±ï¼ˆç”¨æ–¼æ—¥èªŒï¼‰
            max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼ˆé è¨­ 3 æ¬¡ï¼‰
            
        Returns:
            WAV éŸ³é »çš„ bytes
        """
        import time
        import random
        
        voice_name = VOICE_OPTIONS.get(voice, "Kore")
        
        # ============ æ§‹å»ºå¸¶æƒ…ç·’æ§åˆ¶çš„ TTS Prompt ============
        # ä½¿ç”¨ Google å®˜æ–¹æ¨è–¦çš„ Audio Profile æ ¼å¼
        styled_prompt = self._build_healing_tts_prompt(text, part_name)
        
        # ============ é™¤éŒ¯ï¼šé¡¯ç¤ºè«‹æ±‚è³‡è¨Š ============
        print(f"   [TTS] ğŸ” é™¤éŒ¯è³‡è¨Š:")
        print(f"   [TTS]    ç‰‡æ®µ: {part_name}")
        print(f"   [TTS]    æ–‡å­—é•·åº¦: {len(text)} å­—")
        print(f"   [TTS]    è²éŸ³: {voice_name}")
        print(f"   [TTS]    æ¨¡å‹: {TTS_MODEL}")
        print(f"   [TTS]    ğŸ­ å·²åŠ å…¥æƒ…ç·’æ§åˆ¶æŒ‡ä»¤")
        
        last_error = None
        
        for attempt in range(max_retries + 1):
            try:
                print(f"   [TTS] æ­£åœ¨ç™¼é€ TTS è«‹æ±‚... (å˜—è©¦ {attempt + 1}/{max_retries + 1})")
                
                # ============ ä½¿ç”¨å¸¶é¢¨æ ¼çš„ prompt ============
                response = self.client.models.generate_content(
                    model=TTS_MODEL,
                    contents=styled_prompt,  # ä½¿ç”¨å¸¶æƒ…ç·’æ§åˆ¶çš„å®Œæ•´ prompt
                    config=types.GenerateContentConfig(
                        response_modalities=["AUDIO"],
                        speech_config=types.SpeechConfig(
                            voice_config=types.VoiceConfig(
                                prebuilt_voice_config=types.PrebuiltVoiceConfig(
                                    voice_name=voice_name,
                                )
                            ),
                        ),
                    )
                )
                
                print(f"   [TTS] ğŸ“¥ æ”¶åˆ°å›æ‡‰")
                
                # å®‰å…¨ç²å– PCM æ•¸æ“š
                if not response.candidates:
                    raise ValueError("TTS å›æ‡‰æ²’æœ‰ candidates")
                
                print(f"   [TTS]    å€™é¸è€…æ•¸é‡: {len(response.candidates)}")
                
                candidate = response.candidates[0]
                if not hasattr(candidate, 'content') or not candidate.content:
                    raise ValueError("TTS å›æ‡‰æ²’æœ‰ content")
                
                if not candidate.content.parts:
                    raise ValueError("TTS å›æ‡‰æ²’æœ‰ parts")
                
                print(f"   [TTS]    parts æ•¸é‡: {len(candidate.content.parts)}")
                
                part = candidate.content.parts[0]
                if not hasattr(part, 'inline_data') or not part.inline_data:
                    # æª¢æŸ¥æ˜¯å¦æœ‰æ–‡å­—å›æ‡‰ï¼ˆéŒ¯èª¤æƒ…æ³ï¼‰
                    if hasattr(part, 'text') and part.text:
                        print(f"   [TTS] âš ï¸ æ”¶åˆ°æ–‡å­—å›æ‡‰è€ŒééŸ³é »: {part.text[:100]}...")
                    raise ValueError("TTS å›æ‡‰æ²’æœ‰ inline_data")
                
                pcm_data = part.inline_data.data
                if not pcm_data:
                    raise ValueError("TTS å›æ‡‰çš„éŸ³é »æ•¸æ“šç‚ºç©º")
                
                # è½‰æ›ç‚º WAV
                wav_data = self._pcm_to_wav(pcm_data)
                
                print(f"   [TTS] âœ… {part_name} ç”Ÿæˆå®Œæˆ ({len(wav_data)} bytes)")
                return wav_data
                
            except Exception as e:
                last_error = e
                error_str = str(e)
                
                print(f"   [TTS] âŒ éŒ¯èª¤é¡å‹: {type(e).__name__}")
                print(f"   [TTS] âŒ éŒ¯èª¤è¨Šæ¯: {e}")
                
                # ============ å¢å¼·è¨ºæ–· ============
                if "403" in error_str or "PERMISSION_DENIED" in error_str:
                    print(f"   [TTS] ğŸ“ è¨ºæ–·: API æ¬Šé™è¢«æ‹’")
                    print(f"   [TTS]    å»ºè­°: ç¢ºèª GEMINI_API_KEY æœ‰ TTS æ¬Šé™")
                elif "429" in error_str or "RESOURCE_EXHAUSTED" in error_str:
                    print(f"   [TTS] ğŸ“ è¨ºæ–·: API é…é¡è¶…å‡º")
                    print(f"   [TTS]    å»ºè­°: ç­‰å¾…é…é¡é‡ç½®æˆ–å‡ç´šæ–¹æ¡ˆ")
                elif "UNAVAILABLE" in error_str or "INTERNAL" in error_str:
                    print(f"   [TTS] ğŸ“ è¨ºæ–·: TTS æœå‹™æš«æ™‚ä¸å¯ç”¨")
                    print(f"   [TTS]    å»ºè­°: ç¨å¾Œé‡è©¦")
                elif "inline_data" in error_str or "no parts" in error_str.lower():
                    print(f"   [TTS] ğŸ“ è¨ºæ–·: TTS å›æ‡‰æ ¼å¼ç•°å¸¸ï¼ˆå¯èƒ½æ˜¯ Preview æ¨¡å‹å•é¡Œï¼‰")
                    print(f"   [TTS]    å»ºè­°: ç¸®çŸ­æ–‡å­—é•·åº¦æˆ–ç¨å¾Œé‡è©¦")
                # ============ è¨ºæ–·çµæŸ ============
                
                if attempt < max_retries:
                    # æŒ‡æ•¸é€€é¿ + éš¨æ©ŸæŠ–å‹•
                    base_delay = 2 ** attempt  # 1, 2, 4 ç§’
                    jitter = random.uniform(0, 0.5)  # 0-0.5 ç§’éš¨æ©ŸæŠ–å‹•
                    delay = base_delay + jitter
                    
                    print(f"   [TTS] {part_name} ç¬¬ {attempt + 1} æ¬¡å¤±æ•—")
                    print(f"   [TTS] ç­‰å¾… {delay:.1f} ç§’å¾Œé‡è©¦...")
                    time.sleep(delay)
                else:
                    print(f"   [TTS] {part_name} é‡è©¦ {max_retries} æ¬¡å¾Œä»å¤±æ•—")
        
        # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—å¾Œæ‰æ‹‹å‡ºç•°å¸¸
        raise last_error
    
    def _pcm_to_wav(
        self, 
        pcm_data: bytes, 
        channels: int = 1, 
        rate: int = 24000, 
        sample_width: int = 2
    ) -> bytes:
        """å°‡ PCM æ•¸æ“šè½‰æ›ç‚º WAV æ ¼å¼"""
        buffer = BytesIO()
        with wave.open(buffer, "wb") as wf:
            wf.setnchannels(channels)
            wf.setsampwidth(sample_width)
            wf.setframerate(rate)
            wf.writeframes(pcm_data)
        
        buffer.seek(0)
        return buffer.read()
    
    def stitch_audio_clips(
        self, 
        audio_clips: List[bytes],
        silence_duration_ms: int = 800
    ) -> bytes:
        """
        å°‡å¤šå€‹éŸ³é »ç‰‡æ®µç„¡ç¸«ç¸«åˆ
        
        Args:
            audio_clips: WAV æ ¼å¼çš„éŸ³é »ç‰‡æ®µåˆ—è¡¨
            silence_duration_ms: ç‰‡æ®µä¹‹é–“çš„éœéŸ³æ™‚é•·ï¼ˆæ¯«ç§’ï¼‰
            
        Returns:
            åˆä½µå¾Œçš„ WAV éŸ³é » bytes
        """
        print(f"ğŸ“[Audio Stitcher] æ­£åœ¨ç¸«åˆ {len(audio_clips)} å€‹éŸ³é »ç‰‡æ®µ...")
        
        if PYDUB_AVAILABLE:
            return self._stitch_with_pydub(audio_clips, silence_duration_ms)
        else:
            return self._stitch_basic_wav(audio_clips)
    
    def _stitch_with_pydub(
        self, 
        audio_clips: List[bytes],
        silence_duration_ms: int = 800
    ) -> bytes:
        """ä½¿ç”¨ pydub ç¸«åˆéŸ³é »ï¼ˆæ”¯æ´æ·¡å…¥æ·¡å‡ºï¼‰"""
        combined = AudioSegment.empty()
        
        for i, clip_data in enumerate(audio_clips):
            # å¾ bytes è¼‰å…¥éŸ³é »
            clip = AudioSegment.from_wav(BytesIO(clip_data))
            
            # æ·»åŠ æ·¡å…¥ï¼ˆé¦–ç‰‡æ®µï¼‰å’Œæ·¡å‡ºï¼ˆå°¾ç‰‡æ®µï¼‰æ•ˆæœ
            if i == 0:
                clip = clip.fade_in(500)  # 500ms æ·¡å…¥
            if i == len(audio_clips) - 1:
                clip = clip.fade_out(1000)  # 1000ms æ·¡å‡º
            
            # ç¸«åˆ
            if i > 0:
                # åœ¨ç‰‡æ®µä¹‹é–“æ·»åŠ çŸ­æš«éœéŸ³éæ¸¡
                silence = AudioSegment.silent(duration=silence_duration_ms)
                combined += silence
            
            combined += clip
        
        # å°å‡ºç‚º WAV
        output_buffer = BytesIO()
        combined.export(output_buffer, format="wav")
        output_buffer.seek(0)
        
        print(f"   âœ… éŸ³é »ç¸«åˆå®Œæˆï¼ç¸½æ™‚é•·: {len(combined) / 1000:.1f} ç§’")
        return output_buffer.read()
    
    def _stitch_basic_wav(self, audio_clips: List[bytes]) -> bytes:
        """åŸºç¤ WAV æ‹¼æ¥ï¼ˆä¸éœ€è¦ pydubï¼‰"""
        if not audio_clips:
            return b""
        
        # è®€å–ç¬¬ä¸€å€‹æª”æ¡ˆç²å–åƒæ•¸
        first_clip = BytesIO(audio_clips[0])
        with wave.open(first_clip, 'rb') as wf:
            params = wf.getparams()
            sample_rate = wf.getframerate()
            channels = wf.getnchannels()
            sample_width = wf.getsampwidth()
        
        # åˆä½µæ‰€æœ‰ PCM æ•¸æ“š
        all_frames = b""
        for clip_data in audio_clips:
            clip_buffer = BytesIO(clip_data)
            with wave.open(clip_buffer, 'rb') as wf:
                frames = wf.readframes(wf.getnframes())
                all_frames += frames
                # æ·»åŠ  0.5 ç§’éœéŸ³
                silence_frames = b'\x00' * int(sample_rate * channels * sample_width * 0.5)
                all_frames += silence_frames
        
        # å¯«å…¥æ–°çš„ WAV
        output_buffer = BytesIO()
        with wave.open(output_buffer, 'wb') as wf:
            wf.setnchannels(channels)
            wf.setsampwidth(sample_width)
            wf.setframerate(sample_rate)
            wf.writeframes(all_frames)
        
        output_buffer.seek(0)
        print(f"   âœ… åŸºç¤ WAV æ‹¼æ¥å®Œæˆ")
        return output_buffer.read()
    
    def create_streaming_bgm_mixer(self, stage2_result: Dict[str, Any] = None):
        """
        å‰µå»ºä¸²æµ BGM æ··åˆå™¨ï¼ˆç”¨æ–¼å³æ™‚æ··åˆæ¯å€‹ TTS ç‰‡æ®µï¼‰
        
        Returns:
            StreamingBGMMixer å¯¦ä¾‹
        """
        return StreamingBGMMixer(stage2_result)
    
    def _apply_bgm_mixing(
        self, 
        voice_audio: bytes,
        stage2_result: Dict[str, Any]
    ) -> Tuple[bytes, Dict[str, Any]]:
        """
        å°‡èªéŸ³èˆ‡èƒŒæ™¯éŸ³æ¨‚æ··åˆ
        
        Args:
            voice_audio: èªéŸ³éŸ³é » bytes
            stage2_result: äºŒéšåˆ†æçµæœï¼ˆç”¨æ–¼æå–æƒ…ç·’ï¼‰
            
        Returns:
            Tuple of:
            - æ··åˆå¾Œçš„éŸ³é » bytesï¼ˆå¦‚æœç„¡ BGM å‰‡è¿”å›åŸèªéŸ³ï¼‰
            - BGM ç‹€æ…‹å­—å…¸ {"success": bool, "method": str, "error": str|None}
        """
        print("\n" + "=" * 50)
        print("ğŸµ é–‹å§‹ BGM æ··éŸ³æµç¨‹")
        print("=" * 50)
        
        bgm_status = {
            "success": False,
            "method": "none",
            "error": None,
            "voice_only": True
        }
        
        try:
            from conflict_analyzer.audio_mixer import AudioMixer
            
            # å¾ stage2 æå–æƒ…ç·’æ¨™ç±¤
            emotion = "healing"  # é è¨­ç‚ºç™‚ç™’
            if isinstance(stage2_result, dict):
                # å˜—è©¦å¾ä¸åŒæ¬„ä½æå–æƒ…ç·’
                if stage2_result.get("sentiment_vibe"):
                    emotion = stage2_result["sentiment_vibe"]
                elif stage2_result.get("attachment_dynamic"):
                    # å¾ä¾é™„å‹•æ…‹ä¸­æå–é—œéµè©
                    dynamic = str(stage2_result["attachment_dynamic"]).lower()
                    if any(word in dynamic for word in ["ç„¦æ…®", "anxiety"]):
                        emotion = "calm"
                    elif any(word in dynamic for word in ["æ‚²å‚·", "sad"]):
                        emotion = "sadness"
                    elif any(word in dynamic for word in ["ææ‡¼", "fear"]):
                        emotion = "fear"
                    elif any(word in dynamic for word in ["è„†å¼±", "vulnerable"]):
                        emotion = "vulnerability"
            
            print(f"ğŸ“[BGM Mixing] æƒ…ç·’æ¨™ç±¤: {emotion}")
            print(f"ğŸ“[BGM Mixing] èªéŸ³å¤§å°: {len(voice_audio)} bytes")
            
            # åˆå§‹åŒ–æ··éŸ³å™¨ï¼ˆä¸éœ€è¦è‡ªå‹•ä¸‹è¼‰ï¼Œå› ç‚ºæœƒä½¿ç”¨ Lyriaï¼‰
            mixer = AudioMixer(auto_download=False)
            
            # å„ªå…ˆä½¿ç”¨ Lyria ç”ŸæˆåŸå‰µ BGM
            # å¦‚æœ Lyria å¤±æ•—ï¼Œæœƒè‡ªå‹•é™ç´šåˆ°æœ¬åœ° BGM
            print("ğŸ“[BGM Mixing] å˜—è©¦ä½¿ç”¨ Lyria ç”ŸæˆåŸå‰µ BGM...")
            
            mixed_audio = mixer.mix_voice_with_lyria(
                voice_bytes=voice_audio,
                emotion=emotion,
                voice_format="wav"
            )
            
            # æª¢æŸ¥æ··éŸ³æ˜¯å¦çœŸçš„æˆåŠŸï¼ˆæ¯”è¼ƒå¤§å°ï¼‰
            if len(mixed_audio) > len(voice_audio) * 1.1:  # æ··å…¥ BGM å¾Œæ‡‰è©²æ›´å¤§
                bgm_status = {
                    "success": True,
                    "method": "lyria",
                    "error": None,
                    "voice_only": False
                }
                print(f"âœ… [BGM Mixing] æ··éŸ³å®Œæˆï¼è¼¸å‡ºå¤§å°: {len(mixed_audio)} bytes")
            else:
                bgm_status = {
                    "success": False,
                    "method": "fallback",
                    "error": "æ··éŸ³è¼¸å‡ºå¤§å°ç•°å¸¸ï¼Œå¯èƒ½ä½¿ç”¨ç´”èªéŸ³",
                    "voice_only": True
                }
                print(f"âš ï¸ [BGM Mixing] æ··éŸ³å¯èƒ½æœªæˆåŠŸï¼ˆè¼¸å‡ºå¤§å°: {len(mixed_audio)} vs åŸå§‹: {len(voice_audio)}ï¼‰")
            
            return mixed_audio, bgm_status
            
        except ImportError as e:
            error_msg = f"AudioMixer æ¨¡çµ„è¼‰å…¥å¤±æ•—: {e}"
            print(f"âš ï¸ {error_msg}")
            print("   é€™å¯èƒ½æ˜¯å› ç‚º pydub æœªå®‰è£")
            print("   è¿”å›ç´”èªéŸ³ï¼ˆç„¡èƒŒæ™¯éŸ³æ¨‚ï¼‰")
            bgm_status = {
                "success": False,
                "method": "none",
                "error": error_msg,
                "voice_only": True
            }
            return voice_audio, bgm_status
        except Exception as e:
            error_msg = f"{type(e).__name__}: {e}"
            print(f"\nğŸš¨ [BGM Mixing] æ··éŸ³éç¨‹å¤±æ•—!")
            print(f"   éŒ¯èª¤é¡å‹: {type(e).__name__}")
            print(f"   éŒ¯èª¤è¨Šæ¯: {e}")
            print("   ğŸ“ è¨ºæ–·å»ºè­°ï¼š")
            print("      1. æŸ¥çœ‹ä¸Šæ–¹çš„ Lyria API éŒ¯èª¤è¨Šæ¯")
            print("      2. ç¢ºèª GEMINI_API_KEY æœ‰ Lyria éŸ³æ¨‚ç”Ÿæˆæ¬Šé™")
            print("      3. æª¢æŸ¥ assets/bgm/ è³‡æ–™å¤¾æ˜¯å¦æœ‰ MP3/WAV æª”æ¡ˆ")
            print("      4. æª¢æŸ¥ç¶²è·¯é€£ç·šæ˜¯å¦æ­£å¸¸")
            print("   è¿”å›ç´”èªéŸ³ï¼ˆç„¡èƒŒæ™¯éŸ³æ¨‚ï¼‰")
            bgm_status = {
                "success": False,
                "method": "none",
                "error": error_msg,
                "voice_only": True
            }
            return voice_audio, bgm_status
    
    def generate_healing_audio(
        self,
        stage1_result: Dict[str, Any],
        stage2_result: Dict[str, Any],
        stage3_result: Dict[str, Any],
        system_prompt: str,
        voice: str = "warm_female",
        output_dir: Optional[Path] = None,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        å®Œæ•´æµç¨‹ï¼šç”Ÿæˆåˆ†æ®µç™‚è‚²æ–‡ç¨¿ä¸¦ä¸²æ¥ç‚ºå®Œæ•´éŸ³é »
        
        æµç¨‹ï¼š
        1. ç”Ÿæˆå¸¶æœ‰ [PART_X] æ¨™ç±¤çš„æ–‡ç¨¿
        2. æ‹†åˆ†æ–‡ç¨¿ç‚ºå¤šå€‹ç‰‡æ®µ
        3. é †åºç”Ÿæˆæ¯å€‹ç‰‡æ®µçš„éŸ³é »
        4. ä½¿ç”¨ pydub ç¸«åˆæ‰€æœ‰ç‰‡æ®µ
        
        Args:
            stage1_result: ä¸€éšåˆ†æçµæœ
            stage2_result: äºŒéšåˆ†æçµæœ
            stage3_result: ä¸‰éšåˆ†æçµæœ
            system_prompt: ç¬¬å››éš System Prompt
            voice: è²éŸ³é¸é …
            output_dir: å¯é¸çš„è¼¸å‡ºç›®éŒ„
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸ (current, total, message)
            
        Returns:
            {
                "script": str,           # å®Œæ•´æ–‡ç¨¿
                "audio_base64": str,     # Base64 ç·¨ç¢¼çš„ WAV éŸ³é »
                "duration_estimate": float,  # ä¼°ç®—æ™‚é•·ï¼ˆç§’ï¼‰
                "voice": str,            # ä½¿ç”¨çš„è²éŸ³
                "parts_count": int       # ç‰‡æ®µæ•¸é‡
            }
        """
        print("\n" + "=" * 50)
        print("ğŸµ é–‹å§‹ç”Ÿæˆç™‚è‚²éŸ³é »ï¼ˆåˆ†æ®µä¸²æ¥æ¨¡å¼ï¼‰")
        print("=" * 50)
        
        # 1. ç”Ÿæˆæ–‡ç¨¿
        if progress_callback:
            progress_callback(1, 6, "æ­£åœ¨ç”Ÿæˆç™‚è‚²æ–‡ç¨¿...")
        
        script = self.generate_healing_script(
            stage1_result,
            stage2_result,
            stage3_result,
            system_prompt
        )
        
        # 2. æ‹†åˆ†æ–‡ç¨¿
        if progress_callback:
            progress_callback(2, 6, "æ­£åœ¨æ‹†åˆ†æ–‡ç¨¿ç‰‡æ®µ...")
        
        parts = split_script_by_parts(script)
        total_parts = len(parts)
        
        # 3. é †åºç”Ÿæˆæ¯å€‹ç‰‡æ®µçš„éŸ³é »ï¼ˆå¸¶æ–·é»çºŒå‚³å’Œç‹€æ…‹è¿½è¹¤ï¼‰
        print(f"\n[Sequential TTS] é–‹å§‹é †åºç”Ÿæˆ {total_parts} å€‹éŸ³é »ç‰‡æ®µï¼ˆå«è‡ªå‹•é‡è©¦ï¼‰...")
        
        audio_clips = []
        failed_parts = []
        successful_parts = []
        
        for i, (part_name, content) in enumerate(parts, 1):
            # æ›´æ–°é€²åº¦ï¼ˆæ¯å€‹ç‰‡æ®µç¨ç«‹è¿½è¹¤ï¼‰
            if progress_callback:
                progress_callback(
                    2 + i, 
                    2 + total_parts + 2,  # æ–‡ç¨¿ + æ‹†åˆ† + æ¯å€‹ç‰‡æ®µ + ç¸«åˆ + æ··éŸ³
                    f"æ­£åœ¨ç”ŸæˆéŸ³é »ç‰‡æ®µ {i}/{total_parts}..."
                )
            
            try:
                audio_data = self.text_to_speech_single(content, voice, part_name)
                audio_clips.append(audio_data)
                successful_parts.append(part_name)
                print(f"   [é€²åº¦] å·²å®Œæˆ {len(successful_parts)}/{total_parts} å€‹ç‰‡æ®µ")
            except Exception as e:
                failed_parts.append({"part": part_name, "error": str(e)})
                print(f"   [è·³é] {part_name} æœ€çµ‚ç”Ÿæˆå¤±æ•—: {e}")
                # ç¹¼çºŒè™•ç†ä¸‹ä¸€å€‹ç‰‡æ®µï¼ˆæ–·é»çºŒå‚³åŸå‰‡ï¼‰
                continue
        
        # çµ±è¨ˆçµæœ
        success_rate = len(successful_parts) / total_parts * 100 if total_parts > 0 else 0
        print(f"\n[TTS çµ±è¨ˆ] æˆåŠŸ: {len(successful_parts)}/{total_parts} ({success_rate:.0f}%)")
        
        if failed_parts:
            print(f"[TTS çµ±è¨ˆ] å¤±æ•—ç‰‡æ®µ: {[p['part'] for p in failed_parts]}")
        
        # å±€éƒ¨å¯ç”¨æ€§ï¼šå³ä½¿éƒ¨åˆ†å¤±æ•—ä¹Ÿè¿”å›å·²å®Œæˆçš„éƒ¨åˆ†
        if not audio_clips:
            raise Exception("æ‰€æœ‰éŸ³é »ç‰‡æ®µç”Ÿæˆå¤±æ•—ï¼Œç„¡æ³•ç”¢ç”Ÿä»»ä½•éŸ³é »")
        
        # 4. ç¸«åˆéŸ³é »
        if progress_callback:
            progress_callback(4, 5, "æ­£åœ¨ç·¨ç¹”æ‚¨çš„å°ˆå±¬ç™‚ç™’èƒ½é‡...")
        
        stitched_audio = self.stitch_audio_clips(audio_clips)
        
        # 5. æ··éŸ³ï¼šåŠ å…¥èƒŒæ™¯éŸ³æ¨‚ (å¦‚æœå¯ç”¨)
        if progress_callback:
            progress_callback(5, 5, "æ­£åœ¨èåˆç™‚ç™’æ°›åœéŸ³æ¨‚...")
        
        final_audio, bgm_status = self._apply_bgm_mixing(stitched_audio, stage2_result)
        
        # å„²å­˜ï¼ˆå¦‚æœæŒ‡å®šäº†ç›®éŒ„ï¼‰
        if output_dir:
            output_dir = Path(output_dir)
            output_dir.mkdir(exist_ok=True)
            output_path = output_dir / "healing_audio.wav"
            with open(output_path, "wb") as f:
                f.write(final_audio)
            print(f"[å„²å­˜] å·²å„²å­˜: {output_path}")
        
        # è¨ˆç®—å®Œæˆåº¦
        is_complete = len(failed_parts) == 0
        completion_rate = len(successful_parts) / total_parts * 100 if total_parts > 0 else 0
        
        print("\n" + "=" * 50)
        if is_complete:
            print("[å®Œæˆ] ç™‚è‚²éŸ³é »ç”Ÿæˆå®Œæˆï¼ˆ100%ï¼‰")
        else:
            print(f"[éƒ¨åˆ†å®Œæˆ] ç™‚è‚²éŸ³é »ç”Ÿæˆ {completion_rate:.0f}%ï¼ˆ{len(failed_parts)} å€‹ç‰‡æ®µå¤±æ•—ï¼‰")
        print(f"   - æˆåŠŸç‰‡æ®µ: {len(successful_parts)}/{total_parts}")
        print(f"   - ç¸½é•·åº¦: {len(final_audio)} bytes")
        print(f"   - BGM ç‹€æ…‹: {bgm_status.get('method', 'unknown')}")
        print("=" * 50 + "\n")
        
        return {
            "script": script,
            "audio_base64": base64.b64encode(final_audio).decode("utf-8"),
            "duration_estimate": len(script) * 0.12,  # ä¼°ç®—æ™‚é•·ï¼ˆç§’ï¼‰
            "voice": voice,
            "parts_count": len(successful_parts),
            "total_parts": total_parts,
            "failed_parts": failed_parts,  # æ–°å¢ï¼šå¤±æ•—ç‰‡æ®µè©³æƒ…
            "is_complete": is_complete,     # æ–°å¢ï¼šæ˜¯å¦å®Œæ•´
            "completion_rate": completion_rate,  # æ–°å¢ï¼šå®Œæˆç‡
            "bgm_status": bgm_status
        }


# ä¾¿æ·å‡½æ•¸
def generate_healing_audio_from_analysis(
    stage1: Dict[str, Any],
    stage2: Dict[str, Any],
    stage3: Dict[str, Any],
    system_prompt: str,
    voice: str = "warm_female"
) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•¸ï¼šå¾åˆ†æçµæœç”Ÿæˆç™‚è‚²éŸ³é »ï¼ˆè‡ªå‹•åˆ†æ®µä¸²æ¥ï¼‰
    
    Returns:
        {
            "script": str,           # å®Œæ•´æ–‡ç¨¿
            "audio_base64": str,     # Base64 ç·¨ç¢¼çš„ WAV éŸ³é »
            "duration_estimate": float,  # ä¼°ç®—æ™‚é•·
            "voice": str,            # ä½¿ç”¨çš„è²éŸ³
            "parts_count": int       # ç‰‡æ®µæ•¸é‡
        }
    """
    generator = HealingAudioGenerator()
    return generator.generate_healing_audio(
        stage1_result=stage1,
        stage2_result=stage2,
        stage3_result=stage3,
        system_prompt=system_prompt,
        voice=voice
    )
